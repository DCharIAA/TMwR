```{r prediction-trust-setup, include = FALSE}
knitr::opts_chunk$set(fig.path = "figures/")
library(patchwork)
library(tidymodels)
library(applicable)
library(probably)
```

# Trusting your predictions  {#trust}

The nature of most predictive models is that, barring any missing information, they can almost always produce a prediction. However, there will be occasions where it is **inappropriate** to do so. For example, extrapolation occurs when the new data point is well outside of the range of data used to create the model. A more qualitative example of an inappropriate prediction occurs when the model is used in a completely different context. The cell segmentation data used previously flags when human breast cancer cells cannot be accurately quantified. This model could be inappropriately applied to stomach cells for the same purpose. We can produce a prediction but it is unlikely to be _applicable_ to the new cell type.

This chapter discussed two methods for quantifying the potential quality of a prediction. The first uses the predicted values to alert the user that the prediction may be suspect. The second approach uses the predictors to measure the amount of extrapolation (if any) when predicting new samples. 

## Equivocal Results {#equivocal-zones}

In some cases the amount of uncertainty associated with a prediction is too high to be trusted. For example, if you had a model result that indicated that you had a 51% chance of having contracted COVID it would be natural to view the diagnosis with some skepticism. In fact, regulatory bodies often require many medical diagnostics to have an _equivocal zone_. This is a range of results that indicate that the result  should not be reported to patients. The same notion can be applied to many model predictions. 

Consider the two class problem shown in Section \@ref(what-to-optimize) which included a visualization of linear class boundaries for the test set. The data points closest to the class boundary are the most uncertain. If their values changed slightly, their predicted class might have changed. One simple method for disqualifying some results is to call them equivocal if the values are within some range around 50% (or whatever the appropriate probability cutoff might be). Depending on the problem that the model is being applied, this might indicate that another measurement should be collected or that we require more information before a trustworthy prediction is possible.  

We could base the width of the band on how performance improves when the uncertain results are removed. However, we should also estimate the reportable rate (the expected proportion of acceptable results). For example, it would be problematic to have perfect performance but only release predictions on 2% of the samples given to the model. 

Using the previous training and test split, we estimated the logistic regression model using Bayesian methods (using the default Gaussian prior distributions for the parameters): 

```{r prediction-trust-bayes-glm}
library(tidymodels)
data(two_class_dat, package = "modeldata")

set.seed(91)
split <- initial_split(two_class_dat)

training_set <- training(split)
testing_set  <-  testing(split)

two_class_mod <- 
   logistic_reg() %>% 
   set_engine("stan", seed = 1601) %>% 
   fit(Class ~ ., data = training_set)
print(two_class_mod, digits = 3)
```

While a simple logistic model could have been used here to achieve virtually the same results, we'll demonstrate the advantage of the Bayesian approach below. 

The figure below shows the estimated mean probability for being the first class from this model (with the test set points overlaid). 

```{r prediction-trust-glm-grid, echo = FALSE, fig.width=7, fig.height=7.5, out.width="80%"}
data_grid <- crossing(A = seq(0.4, 4, length = 200), B = seq(.14, 3.9, length = 200))

grid_pred <- 
   predict(two_class_mod, data_grid, type = "prob") %>% 
   bind_cols(
      predict(two_class_mod, data_grid, type = "pred_int", std_error = TRUE),
      data_grid
   )

grid_pred %>% 
   mutate(`Probability of Class 1` = .pred_Class1) %>% 
   ggplot(aes(x = A, y = B)) + 
   geom_raster(aes(fill = `Probability of Class 1`))+ 
   scale_fill_gradient2(low = "#E66101", mid = "white", high = "#5E3C99", midpoint = .5) +
   geom_point(data = testing_set, aes(shape = Class), alpha = .5, size = 2) + 
   geom_contour(aes(z = .pred_Class1), breaks = .5, col = "black", lty = 2) + 
   coord_equal() + 
   labs(x = "Predictor A", y = "Predictor B") + 
   theme_bw() + 
   theme(legend.position = "top", legend.direction = "vertical")
```

With tidymodels, the `r pkg(probably)` package contains functions for equivocal zones. 

```{r trust-probably}
equiv_pred <-
  predict(two_class_mod, training_set, type = "prob") %>%
  bind_cols(training_set) %>% 
  mutate(
    .pred_equiv = make_two_class_pred(
      .pred_Class1,
      levels = c("Class1", "Class2"),
      buffer = 0.05
    )
  )
equiv_pred
```



As an example, the test set can be used to determine the balance between improving performance and reportable results. The predictions are created using:  

```{r prediction-trust-bayes-glm-pred}
test_pred <- 
   predict(two_class_mod, testing_set, type = "prob") %>% 
   bind_cols(predict(two_class_mod, testing_set)) %>% 
   bind_cols(testing_set)
test_pred %>% head()
```
```{r eq-calcs, include = FALSE}
reportable <- function(x, dat) {
  dat %>% 
    mutate(
    .pred_equiv = make_two_class_pred(
      .pred_Class1,
      levels = c("Class1", "Class2"),
      buffer = x
    )
  ) %>% pluck(".pred_equiv") %>% 
    reportable_rate()

}
acc_results <- function(x, dat) {
  dat %>% 
    mutate(
      .pred_equiv = make_two_class_pred(
        .pred_Class1,
        levels = c("Class1", "Class2"),
        buffer = x
      )
    ) %>% 
    filter(!is_equivocal(.pred_equiv)) %>% 
    accuracy(Class, .pred_class) %>% 
    pluck(".estimate")
}
eq_results <- 
   tibble(buffer = seq(0, 0.1, length.out = 50)) %>% 
   mutate(
      reportable = map_dbl(buffer, reportable, dat = test_pred),
      accuracy = map_dbl(buffer, acc_results, dat = test_pred)
   )
two_pct <- eq_results$buffer[which.min(abs(eq_results$reportable - .98))]
two_pct_imp <- 
   eq_results$accuracy[which.min(abs(eq_results$reportable - .98))] - 
   eq_results$accuracy[eq_results$buffer == 0]
```

The figure below shows how the accuracy changes when we apply buffers around 50% of various sizes. For example, if we determined that not reporting 2% of the results was acceptable, predictions with probabilities outside of 50%$\pm$ `r two_pct` would be labeled as equivocal. 

```{r prediction-trust-eq-grid, echo = FALSE, fig.height=4, out.width="80%"}
eq_results %>% 
   pivot_longer(c(-buffer), names_to = "statistic", values_to = "value") %>% 
   ggplot(aes(x = buffer, y = value, col = statistic)) + 
   geom_line() + 
   labs(y = NULL, x = "50% +/- buffer")
```

The improvement to accuracy is `r signif(two_pct_imp, 3)`, which is unlikely to be important enough to necessitate the zone. 

This analysis focused on using the predicted class probability to disqualify points since this is a fundamental measure of uncertainty in classification models. A slightly better approach would be to use the standard error of the class probability. Since we used a Bayesian model, the probability estimates shown above are actually the mean of the posterior predictive distribution. In other words, the model gives us a distribution for the class probability.  Measuring the standard deviation of this distribution gives us a _standard error of prediction_ of the probability. In most cases, this value is directly related to the mean class probability. You might recall that the variance of a Bernoulli random variable with probability $p$ is $p(1-p)$. Because of this relationship, the standard error is largest when the probability is 50%. Instead of assigning an equivocal result using the class probability, we could instead use a cutoff on the standard error of prediction. 

One important aspect of the standard error of prediction is that it takes into account more than just the class probability. In cases where there is significant extrapolation or aberrant predictor values, the standard error might increase. The benefit of using the standard error of prediction is that it might also flag predictions that are problematic (as opposed to simply uncertain). One reason that we used the Bayesian model is that it naturally estimates the standard error of prediction; not many models can calculate this. For our test set, using `type = "pred_int"` will produce upper and lower limits and the `std_error` adds a column for that quantity. For 80% intervals: 

```{r prediction-trust-pred-int}
test_pred <- 
  bind_cols(
    test_pred,
    predict(two_class_mod, testing_set, type = "pred_int", std_error = TRUE)
  )
```

For our example, where the model and data are well-behaved, this figure shows the standard error of prediction across the space: 

```{r prediction-trust-glm-grid-std-err, echo = FALSE, fig.width=7, fig.height=7.5, out.width="80%"}
grid_pred %>% 
   mutate(`Std Error` = .std_error) %>% 
   ggplot(aes(x = A, y = B)) + 
   geom_raster(aes(fill = `Std Error`)) + 
   scale_fill_gradientn(colours = scales::brewer_pal(palette = "Blues")(6)) + 
   geom_point(data = testing_set, aes(shape = Class), alpha = .5, size = 2) + 
   coord_equal() + 
   labs(x = "Predictor A", y = "Predictor B") + 
   theme_bw() + 
   theme(legend.position = "top", legend.direction = "vertical")
```


## Determining model applicability {#applicability-domains}

Equivocal zones try to measure the reliability of a prediction based on the model outputs. It may be that model statistics, such as predictions or the standard error of prediction, cannot measure the impact of extrapolation. Let's take the Chicago train data described in [Kuhn and Johnson (2019)](https://bookdown.org/max/FES/chicago-intro.html). The goal is to predict the number of customers entering the Clark and Lake train station each day. 

The data set in the `r pkg(modeldata)` package has daily values between `r format(min(Chicago$date), "%B %d %Y")` and `r format(max(Chicago$date), "%B %d %Y")`. A small test set is created using the last two weeks of the data: 

```{r prediction-trust-chicago-data}
data(Chicago)

Chicago <- Chicago %>% select(ridership, date, one_of(stations))

n <- nrow(Chicago)

Chicago_train <- Chicago %>% slice(1:(n - 14))
Chicago_test  <- Chicago %>% slice((n - 13):n)
```

The main predictors are lagged ridership data at different train stations, including Clark and Lake, as well as the date. The ridership predictors are highly correlated with one another. In the recipe below, the date column is expanded into several new features and the ridership predictors are represented using partial least squares (PLS) components. PLS [@Geladi:1986] is a supervised version of principal component analysis where the new features have been decorrelated but are predictive of the outcome data. 

Using the preprocessed data, we fit a standard linear model.

```{r prediction-trust-chicago-model}
base_recipe <-
   recipe(ridership ~ ., data = Chicago_train) %>%
   # Create date features
   step_date(date) %>%
   step_holiday(date) %>%
   # Change date to be an id column instead of a predictor
   update_role(date, new_role = "id") %>%
   # Create dummy variables from factor columns
   step_dummy(all_nominal()) %>%
   # Remove any columns with a single unique value
   step_zv(all_predictors()) %>%
   step_normalize(!!!stations)%>%
   step_pls(!!!stations, num_comp = 10, outcome = vars(ridership))

lm_spec <-
   linear_reg() %>%
   set_engine("lm") 

lm_wflow <-
   workflow() %>%
   add_recipe(base_recipe) %>%
   add_model(lm_spec)

set.seed(1602)
lm_fit <- fit(lm_wflow, data = Chicago_train)
```

How well to the data fit on the test set? 

```{r prediction-trust-chicago-test-res}
res_test <-
   predict(lm_fit, Chicago_test) %>%
   bind_cols(
      predict(lm_fit, Chicago_test, type = "pred_int"),
      Chicago_test
   )

res_test %>% select(date, ridership, starts_with(".pred"))
res_test %>% rmse(ridership, .pred)
```

These are fairly good results. Let's also create a function that adds the day of a week column then visualize the predictions along with 95% prediction intervals: 

```{r prediction-trust-chicago-test-pred, echo = FALSE, fig.height=4, out.width="100%"}
add_day <- function(x) {
   day <- lubridate::wday(x$date, label = TRUE)
   factor(as.character(day), ordered = FALSE, levels = levels(day))
}

res_test %>%
   mutate(day = add_day(.)) %>%
   ggplot(aes(x = date)) +
   geom_point(aes(y = ridership, col = day)) +
   geom_line(aes(y = .pred)) +
   geom_ribbon(aes(ymin = .pred_lower, ymax = .pred_upper), fill = "blue", alpha = .1) +
   scale_color_brewer(palette = "Set2") +
   scale_x_date(labels = date_format("%B %d, %Y")) +
   labs(x = NULL, y = "Daily Ridership (x1000)")
```

Given the scale of the ridership numbers, these results look particularly good for such a simple model. If this model were deployed, how well would it have done a few years later in June of 2020? The predictions are produced without error:

```{r prediction-trust-chicago-2020-res}
load("RData/Chicago_2020.RData")
res_2020 <-
   predict(lm_fit, Chicago_2020) %>%
   bind_cols(
      predict(lm_fit, Chicago_2020, type = "pred_int"),
      Chicago_2020
   ) 

res_2020 %>% select(date, contains(".pred"))
```

The prediction intervals are about the same width even though these data are well beyond the time period of the original training set. The performance on these data are abysmal: 

```{r prediction-trust-chicago-2020-stats}
res_2020 %>% select(date, ridership, starts_with(".pred"))
res_2020 %>% rmse(ridership, .pred)
```

Visually: 

```{r prediction-trust-chicago-2020-pred, echo = FALSE, fig.height=4, out.width="100%"}
res_2020 %>%
   mutate(day = add_day(.)) %>%
   ggplot(aes(x = date)) +
   geom_point(aes(y = ridership, col = day)) +
   geom_line(aes(y = .pred)) +
   geom_ribbon(aes(ymin = .pred_lower, ymax = .pred_upper), fill = "blue", alpha = .1) +
   scale_color_brewer(palette = "Set2") +
   scale_x_date(labels = date_format("%B %d, %Y")) +
   labs(x = NULL, y = "Daily Ridership (x1000)")
```

It's fairly well known that linear regression confidence and prediction intervals expand as the data become more and more removed from the center of the training set. However, that effect is not severe enough to flag the predictions as being poor. In other words, sometimes the quantities produced by the models cannot measure the quality of prediction very well. 

For important models, this situation can be avoided by having a secondary methodology that can quantify how applicable the model is for any new prediction. There are a variety of methods to compute an _applicability domain model_, such as @netzeva2005current or @gadaleta2016applicability. @10.1371/journal.pone.0225715 also describes similar methods with spatial data. The approach used in this chapter is a fairly simple unsupervised method that attempts to measure how much (if any) a new data point is beyond the training data. 

:::rmdnote
The idea is to accompany a prediction with a score that measures how similar the new point is to the training set.
:::


One method that works well uses a principal component analysis on the numeric predictor values. Before applying it to the Chicago data, we'll illustrate the process using the two class data from the first section. The training set are shown in panel (a) below. 

The first step is to conduct PCA on the training data. The PCA components for the training set are shown in panel(b). Next, using these results, we measure the distance of each training set point to the center of the data (panel (c)). We can then use this _reference distribution_ (panel (d)) to estimate how far away a data point is from the mainstream of the training data.  

```{r prediction-trust-pca-two-class-train, echo = FALSE, fig.width=7, fig.height=7, out.width = "100%"}
pca_rec <- recipe(Class ~ ., data = training_set) %>% 
   step_normalize(A, B) %>% 
   step_pca(A, B, num_comp = 2) %>% 
   prep()

training_pca <- bake(pca_rec, new_data = NULL)
pca_center <- 
   training_pca %>% 
   select(PC1, PC2) %>% 
   summarize(PC1_mean = mean(PC1), PC2_mean = mean(PC2))

training_pca <- 
   cbind(pca_center, training_pca) %>% 
   mutate(
      distance = (PC1 - PC1_mean)^2 + (PC2 - PC2_mean)^2,
      distance = sqrt(distance)
   )


testing_pca <- 
   bake(pca_rec, testing_set) %>% 
   cbind(pca_center) %>% 
   mutate(
      distance = (PC1 - PC1_mean)^2 + (PC2 - PC2_mean)^2,
      distance = sqrt(distance)
   ) %>% 
  arrange(desc(distance)) %>% 
  slice(1)

tr_plot <- 
   training_set%>% 
   ggplot(aes(x = A, y = B)) + 
   geom_point(data = testing_set, alpha = .5, size = 1) + 
   coord_equal() + 
   labs(x = "Predictor A", y = "Predictor B", title = "(a) Training Set")

pca_plot <- training_pca %>% 
   ggplot(aes(x = PC1, y = PC2)) + 
   geom_point(alpha = .5, size = 1) + 
   coord_obs_pred() + 
   labs(x = "Component 1", y = "Component 2", title = "(b) Training Set PCA Scores") 


pca_dist <- 
   training_pca %>% 
   ggplot() + 
   geom_point(aes(x = PC1, y = PC2), alpha = .5, size = .1) +
   geom_segment(aes(x = PC1_mean, y = PC2_mean,
                    xend = PC1, yend = PC2), alpha = .1)  + 
   coord_obs_pred() + 
   labs(x = "Component 1", y = "Component 2", title = "(c) Distances to Center")

dist_hist <-
   training_pca %>%
   ggplot(aes(x = distance)) +
   geom_histogram(bins = 30, col = "white") +
   labs(x = "Distance to Training Set Center", title = "(d) Reference Distribution") 

tr_plot + pca_plot + pca_dist + dist_hist
```

For a new sample, the PCA score is computed along with the distance to the center of the _training set_. The plot below overlays a testing set sample with the PCA distances from the training set. 

```{r prediction-trust-pca-two-class-test, echo = FALSE}
test_pca_dist <- 
   training_pca %>% 
   ggplot() + 
   geom_segment(
      aes(x = PC1_mean, y = PC2_mean, xend = PC1, yend = PC2), 
      alpha = .1
   )  + 
   geom_segment(
      data = testing_pca,
      aes(x = PC1_mean, y = PC2_mean, xend = PC1, yend = PC2), 
      col = "red"
      )  + 
   geom_point(data = testing_pca, aes(x = PC1, y = PC2), col = "red") +
   coord_obs_pred() + 
   labs(x = "Component 1", y = "Component 2", title = "Distances to Training Set Center") + 
   theme_bw() + 
   theme(legend.position = "top")

test_dist_hist <- 
   training_pca %>% 
   ggplot(aes(x = distance)) + 
   geom_histogram(bins = 30, col = "white") + 
   geom_vline(xintercept = testing_pca$distance, col = "red") +
   xlab("Distance to Training Set Center")

test_pca_dist + test_dist_hist
```


This data point lies outside of the much of the training with a distance of `r round(testing_pca$distance, 2)`. It is in the `r round(mean(testing_pca$distance > training_pca$distance) * 100, 1)` percentile of the training set distribution. This quantifies what we can see visually in the plot above; This data point is mot similar to ost of the training set data. The cutoff for when to be concerned about large percentile values is dependent on the context of the problem at hand. 

Would this approach have flagged the 2020 data for the Chicago trains? The `r pkg(applicable)` package can develop an applicability domain model using PCA. As inputs, we'll use the 20 lagged station ridership predictors as inputs into the PCA analysis. There is an additional argument called `threshold`  that determines how many components are used in the distance calculation. For our example, we'll use a large value that indicates that we should use enough components to account for 99% of the variation in the ridership predictors: 

```{r prediction-trust-apd-pca}
library(applicable)
pca_stat <-
  apd_pca( ~ ., data = Chicago_train %>% select(one_of(stations)), threshold = 0.99)
pca_stat
```

The `autoplot()` method plots the reference distribution. It has an option argument for which data to plot. We'll add a value of `distance` to show the training set distance distribution: 

```{r prediction-trust-ref-dist}
autoplot(pca_stat, distance) + xlab("Distance")
```

On the x-axis is the values of the distance and the y-axis is the distribution's percentiles. For example, about `r round(approx(pca_stat$pctls$distance, pca_stat$pctls$percentile, xout = 6)$y, 0)`% of the distances are less than 6.0. 

To compute the percentiles for new data, the `score()` functions functions in the same way as `predict()`: 


```{r prediction-trust-apd-test-scores}
score(pca_stat, Chicago_test) %>% select(starts_with("distance"))
```

These seem fairly reasonable. For the 2020 data: 


```{r prediction-trust-apd-2020-scores}
score(pca_stat, Chicago_2020) %>% select(starts_with("distance"))
```

Some of the distance values are completely outside of the data seen by the model. These should be flagged so that the predictions are _not_ reported or should be taken with a large grain of salt. 

`r pkg(applicable)` also contains other methods, including the calculation of the "hat values" for linear regression. There are also specialized methods for data sets where all of the predictors are binary. This method computes similarity scores between training set data points to define the reference distribution. Also, the focus in this section has been on the issue of extrapolation. It is possible that an applicability domain model could let users know regions inside of the training set space that have poor performance. This is possible but might be difficult to do without a large amount of training set. See @keefer2013interpretable as an example. 

